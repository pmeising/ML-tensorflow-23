{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_qgtyZ9yyWt"
      },
      "source": [
        "# Model Definition and Evaluation\n",
        "## Table of Contents\n",
        "1. [Model Selection](#model-selection)\n",
        "2. [Feature Engineering](#feature-engineering)\n",
        "3. [Hyperparameter Tuning](#hyperparameter-tuning)\n",
        "4. [Implementation](#implementation)\n",
        "5. [Evaluation Metrics](#evaluation-metrics)\n",
        "6. [Comparative Analysis](#comparative-analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wxyJIa6UyyWw"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import files, drive\n",
        "import zipfile\n",
        "import shutil\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, MaxPooling2D, Conv2D\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from collections import Counter\n",
        "import random\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "import json\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfPRN-ykzMHq",
        "outputId": "507c6bcd-b9fd-442e-c6ab-76422453325d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the zip file in Google Drive\n",
        "zip_file_path = '/content/drive/MyDrive/ML_Tensorflow/phytoplankton_labeled.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset')\n",
        "\n",
        "data_dir = '/content/dataset/labeled_20201020'\n",
        "\n",
        "base_dir = '/content/dataset/groups'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQMqPWxty52_",
        "outputId": "ff60bb1f-cf18-4d70-b0dc-7f1bd12064a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished loading the models.\n"
          ]
        }
      ],
      "source": [
        "# Import models\n",
        "\n",
        "model_dir = '/content/drive/MyDrive/ML_Tensorflow/models/'\n",
        "\n",
        "child_models = [load_model(model_dir + 'model_0_9.h5'), load_model(model_dir + 'model_10_19.h5'), load_model(model_dir + 'model_20_29.h5'), load_model(model_dir + 'model_30_39.h5'), load_model(model_dir + 'model_40_49.h5')]\n",
        "\n",
        "parent_model = load_model(model_dir + 'model_parent.h5')\n",
        "\n",
        "print(\"Finished loading the models.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7fvZmcqyyWy"
      },
      "source": [
        "## Model Selection\n",
        "\n",
        "We decided to employ a cascading model architecture with Transfer learning based off of the MobileNetV2 architecture trained on 'imagenet'. The models are all Convolutional Neural Networks that classify the phytoplankton image into classes. Initially the parent model would classify into one of 5 groups, which represents one of 5 child models, that would then be invoked to classify into its one of ten classes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa7c96dWyyWy"
      },
      "source": [
        "## Feature Engineering\n",
        "\n",
        "The exact steps are demonstrated in the respective JupyterNotebooks for each model. However as CNNs extract the features using the model architecture, we are using MobileNetV2's feature extraction capabilities and apply them to our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nih__A-kyyWy",
        "outputId": "30dbe875-f443-4575-b6f5-4c7d2571dbd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images in dataset: 63074\n"
          ]
        }
      ],
      "source": [
        "# Initialize lists to hold image paths and labels\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over each sub-directory in the dataset directory\n",
        "for label_dir in os.listdir(data_dir):\n",
        "    label_dir_full_path = os.path.join(data_dir, label_dir)\n",
        "\n",
        "    # Check if it's a directory\n",
        "    if os.path.isdir(label_dir_full_path):\n",
        "        # Iterate over each image in the sub-directory\n",
        "        for image_file in os.listdir(label_dir_full_path):\n",
        "            # Construct the full path for the image file\n",
        "            image_file_full_path = os.path.join(label_dir_full_path, image_file)\n",
        "\n",
        "            # Check if it's a file and not a directory\n",
        "            if os.path.isfile(image_file_full_path):\n",
        "                # Add the image path and label to the lists\n",
        "                image_paths.append(image_file_full_path)\n",
        "                labels.append(label_dir) # assuming label is the directory name\n",
        "\n",
        "# Now you can split the image paths and labels into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(image_paths, labels, test_size=0.3, stratify=labels, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
        "\n",
        "unique_classes = np.unique(labels)\n",
        "\n",
        "# Generate a dictionary mapping numerical values to class names\n",
        "class_to_idx = {class_name: idx for idx, class_name in enumerate(unique_classes)}\n",
        "\n",
        "# Generate a dictionary mapping numerical values back to class names\n",
        "idx_to_class = {idx: class_name for class_name, idx in class_to_idx.items()}\n",
        "\n",
        "print(f\"Number of images in dataset: {len(image_paths)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddHeXqAlC5nz",
        "outputId": "08b21117-658b-4b59-dbe9-0fd2af58b9b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 9462 validated image filenames belonging to 50 classes.\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Convert the lists into DataFrames\n",
        "train_df = pd.DataFrame({'filepath': X_train, 'label': y_train})\n",
        "val_df = pd.DataFrame({'filepath': X_val, 'label': y_val})\n",
        "test_df = pd.DataFrame({'filepath': X_test, 'label': y_test})\n",
        "\n",
        "# Initialize ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create the generator\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='filepath',\n",
        "    y_col='label',\n",
        "    target_size=(160, 160),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5RKpwg4_Kx0",
        "outputId": "ed7aae1f-14e5-4863-8f03-48943f398b8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 1s 761ms/step\n",
            "Image was predicted to belong to group [3 4 1 1 1 2 0 4 1 1 1 3 3 2 2 1 3 1 0 1 0 2 2 4 1 4 4 0 3 1 4 4].\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "Accuracy on test set: 0.00%\n"
          ]
        }
      ],
      "source": [
        "# Function to predict class for a batch of images\n",
        "def predict_classes(images, parent_model, child_models):\n",
        "    # Predict the group using the parent model\n",
        "    group_probabilities = parent_model.predict(images)\n",
        "    groups = np.argmax(group_probabilities, axis=1)\n",
        "    print(f\"Image was predicted to belong to group {groups}.\")\n",
        "\n",
        "    final_predictions = []\n",
        "\n",
        "    # Iterate over each image and its predicted group\n",
        "    for image, group in zip(images, groups):\n",
        "        # Prepare image for prediction (reshape to include batch dimension)\n",
        "        image_batch = np.expand_dims(image, axis=0)\n",
        "\n",
        "        # Predict the final class using the corresponding child model\n",
        "        final_class_probabilities = child_models[group].predict(image_batch)\n",
        "        final_class = np.argmax(final_class_probabilities, axis=1)[0]\n",
        "        final_predictions.append(final_class)\n",
        "\n",
        "    return final_predictions\n",
        "\n",
        "# Predicting on test set\n",
        "predictions = []\n",
        "for images, _ in test_generator:\n",
        "    batch_predictions = predict_classes(images, parent_model, child_models)\n",
        "    predictions.extend(batch_predictions)\n",
        "\n",
        "    # Break the loop if we have predicted for all images\n",
        "    if len(predictions) >= len(test_df) or len(predictions) >= 5:\n",
        "        break\n",
        "\n",
        "# Truncate predictions to match the length of test set (in case of last partial batch)\n",
        "predictions = predictions[:len(test_df)]\n",
        "\n",
        "# Convert labels back to original format if necessary\n",
        "# e.g., labels = [class_mapping[p] for p in predictions]\n",
        "\n",
        "# Calculate accuracy\n",
        "correct_predictions = sum(p == actual for p, actual in zip(predictions, test_df['label']))\n",
        "accuracy = correct_predictions / len(test_df)\n",
        "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN6pszBQHz3e",
        "outputId": "f8f1371e-709f-46cb-e886-ac85b4feb354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 43ms/step\n",
            "Image classified as belonging to group 3\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "Image classified as belonging to group 4\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "Image classified as belonging to group 1\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Image classified as belonging to group 1\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "Image classified as belonging to group 1\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "Image classified as belonging to group 1\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 67ms/step\n",
            "Image classified as belonging to group 0\n",
            "1/1 [==============================] - 0s 77ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "Image classified as belonging to group 1\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "Image classified as belonging to group 1\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "1/1 [==============================] - 0s 74ms/step\n",
            "Image classified as belonging to group 1\n",
            "1/1 [==============================] - 0s 79ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "Image classified as belonging to group 1\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "Image classified as belonging to group 3\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "Image classified as belonging to group 3\n",
            "1/1 [==============================] - 0s 63ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "Image classified as belonging to group 1\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "Image classified as belonging to group 1\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "Image classified as belonging to group 1\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "Image classified as belonging to group 3\n",
            "1/1 [==============================] - 0s 59ms/step\n",
            "1/1 [==============================] - 0s 52ms/step\n",
            "Image classified as belonging to group 0\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "Image classified as belonging to group 0\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 54ms/step\n",
            "Image classified as belonging to group 1\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "Predicted class: Chaetoceros_sp_single, True class: Oscillatoriales\n",
            "Predicted class: Aphanothece_paralleliformis, True class: Uroglenopsis_sp\n",
            "Predicted class: Ceratoneis_closterium, True class: Cryptophyceae-Teleaulax\n",
            "Predicted class: Chroococcales, True class: Dolichospermum-Anabaenopsis-coiled\n",
            "Predicted class: Chlorococcales, True class: Dolichospermum-Anabaenopsis\n",
            "Predicted class: Centrales_sp, True class: Gymnodinium_like\n",
            "Predicted class: Aphanizomenon_flosaquae, True class: Aphanizomenon_flosaquae\n",
            "Predicted class: Ceratoneis_closterium, True class: Heterocapsa_triquetra\n",
            "Predicted class: Chroococcales, True class: Dolichospermum-Anabaenopsis-coiled\n",
            "Predicted class: Centrales_sp, True class: Cryptophyceae-Teleaulax\n",
            "Predicted class: Chlorococcales, True class: Dolichospermum-Anabaenopsis\n",
            "Predicted class: Chaetoceros_sp_single, True class: Oscillatoriales\n",
            "Predicted class: Chaetoceros_sp_single, True class: Oscillatoriales\n",
            "Predicted class: Ceratoneis_closterium, True class: Heterocapsa_triquetra\n",
            "Predicted class: Ceratoneis_closterium, True class: Cryptomonadales\n",
            "Predicted class: Chlorococcales, True class: Dolichospermum-Anabaenopsis\n",
            "Predicted class: Centrales_sp, True class: Oscillatoriales\n",
            "Predicted class: Chlorococcales, True class: Dolichospermum-Anabaenopsis\n",
            "Predicted class: Aphanizomenon_flosaquae, True class: Aphanizomenon_flosaquae\n",
            "Predicted class: Beads, True class: Cryptophyceae-Teleaulax\n"
          ]
        }
      ],
      "source": [
        "def get_class_name_from_index(index, idx_to_class):\n",
        "    \"\"\"\n",
        "    Get the class name corresponding to a numerical index.\n",
        "\n",
        "    :param index: The numerical index (prediction from the model).\n",
        "    :param idx_to_class: A dictionary mapping indices to class names.\n",
        "    :return: The corresponding class name.\n",
        "    \"\"\"\n",
        "    return idx_to_class.get(index, \"Unknown\")\n",
        "\n",
        "def load_and_preprocess_image(image_path, target_size=(160, 160)):\n",
        "    \"\"\"\n",
        "    Load an image and preprocess it for model prediction.\n",
        "\n",
        "    :param image_path: Path to the image file.\n",
        "    :param target_size: The target size to which the image is resized. Default is (160, 160).\n",
        "    :return: Preprocessed image.\n",
        "    \"\"\"\n",
        "    # Open the image file\n",
        "    with Image.open(image_path) as img:\n",
        "        # Resize the image\n",
        "        img = img.resize(target_size)\n",
        "\n",
        "        # Convert the image to a numpy array\n",
        "        image_array = np.array(img)\n",
        "\n",
        "        # Ensure the image has 3 channels (RGB)\n",
        "        if image_array.ndim == 2:\n",
        "            image_array = np.stack((image_array,)*3, axis=-1)\n",
        "        elif image_array.shape[2] == 4:\n",
        "            image_array = image_array[..., :3]\n",
        "\n",
        "        # Normalize the image\n",
        "        image_array = image_array / 255.0\n",
        "\n",
        "    return image_array\n",
        "\n",
        "# Function to predict the class for a single image\n",
        "def predict_class(image, parent_model, child_models):\n",
        "    # Prepare image for prediction (adding batch dimension)\n",
        "    image_batch = np.expand_dims(image, axis=0)\n",
        "\n",
        "    # Predict the group using the parent model\n",
        "    group_probabilities = parent_model.predict(image_batch)\n",
        "    group = np.argmax(group_probabilities, axis=1)[0]\n",
        "    print(f\"Image classified as belonging to group {group}\")\n",
        "\n",
        "    # Select the child model based on the predicted group\n",
        "    child_model = child_models[group]\n",
        "\n",
        "    # Predict the final class using the child model\n",
        "    final_class_probabilities = child_model.predict(image_batch)\n",
        "    final_class = np.argmax(final_class_probabilities, axis=1)[0]\n",
        "\n",
        "    return final_class\n",
        "\n",
        "# Predict for 5 images from the test set\n",
        "num_samples_to_predict = 20\n",
        "sample_predictions = []\n",
        "\n",
        "for i in range(num_samples_to_predict):\n",
        "    image_path = test_df.iloc[i]['filepath']  # Get the image path\n",
        "    true_label = test_df.iloc[i]['label']  # Get the true label for comparison\n",
        "    image = load_and_preprocess_image(image_path)  # Load and preprocess the image\n",
        "\n",
        "    prediction = predict_class(image, parent_model, child_models)  # Predict the class\n",
        "    sample_predictions.append((image_path, prediction, true_label))  # Store the results\n",
        "\n",
        "# Display the predictions\n",
        "for img_path, pred, true in sample_predictions:\n",
        "    class_name = get_class_name_from_index(pred, idx_to_class)\n",
        "    print(f\"Predicted class: {class_name}, True class: {true}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRuLMJEzyyWz"
      },
      "source": [
        "## Hyperparameter Tuning\n",
        "\n",
        "We found that for our hardware, training for about 15 epochs was feasible as well as achieving great results. We also tested out changes to the learning rate, batch size and target number of images per class. Having too many images per class lead to overfitting for those that we only had less than 30 \"real\" images of. Slowly iterating through a combination of those hyperparameters had us reach 175 images per class.\n",
        "Of course the majority of the hyperparameters were given by MobileNetV2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57RoL7qbyyWz"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "The model implementations can be found [here](./JupyterNotebooks/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvOBh8p3yyW0"
      },
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "We initially focussed on test accuracy for the 5 child models. After evaluating the results, we paid more attention to recall and precision (F1-Score being their combination). Those metrics were super helpful for deciding whether we have potential for optimizing the model further. We visualised the results with a confusion matrix. Those can also be found in the respective JuypterNotebooks:\n",
        "* [Range_0_9](./JupyterNotebooks/MobileNetV2_0_9.ipynb)\n",
        "* [Range_10_19](./JupyterNotebooks/MobileNetV2_10_19.ipynb)\n",
        "* [Range_20_29](./JupyterNotebooks/MobileNetV2_20_29.ipynb)\n",
        "* [Range_30_39](./JupyterNotebooks/MobileNetV2_30_39.ipynb)\n",
        "* [Range_40_49](./JupyterNotebooks/MobileNetV2_40_49.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFD_PTwiyyW0"
      },
      "source": [
        "## Comparative Analysis\n",
        "\n",
        "The overall parent model is performing very poorly, unable to classify into one of 5 groups. However, the models for classification into one of 10 classes outperform the baseline model by far, reaching F1-Scores of +95% in some instances.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
